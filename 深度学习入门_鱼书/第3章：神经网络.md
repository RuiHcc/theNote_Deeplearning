上一章感知机的关键点在于其中的**参数** ，而神经网络的一个重要性质是它可以**自动地从数据中学习到合适的权重参数**，这就很牛逼了

# 3.1 从感知机到神经网络
神经网络和上一章介绍的感知机有很多共同点。这里,我们主要以两者的差异为中心,来介绍神经网络的结构。

**神经网络的结构：** 这里我们把它称为 "2 层网络" (两层权重)
	其中，第 0 层对应输入层,第 1 层对应中间层,第 2 层对应输出层。
![[Pasted image 20250527141229.png| 500]]
实际上,就神经元的连接方式而言,与上一章的感知机并没有任何差异。 ^2cc749

**激活函数登场：** 
	在感知机中，最后输出有一个判断（大于小于0）这里我们把它用一个函数$h(x)$ 来表示
	$$
	\begin{matrix}
	y = h(b + w1x1 + w2x2) \\ 
	if (x<=0) :h(x)=0 \\
	else:h(x)=1
	\end{matrix}
	$$
	这种函数一般称为激活函数(activation function)。如“激活”一词所示,激活函数的作用在于决定如何来激活输入信号的总和，关键是他为整个函数引入了**非线性**。
下面,我们将仔细介绍激活函数。
**注：** 
- “朴素感知机”一般指单层网络,指的是激活函数使用了阶跃函数的模型。
- “多层感知机”是指神经网络,即使用 sigmoid  函数(后述)等平滑的激活函数的多层网络。
- 同时一般认为感知机的参数$W$ 是由人为设定，而神经网络的参数 $W$ 通过学习获取

# 3.2 激活函数
感知机中采用阶跃函数，但是这太过理想了，阶跃函数虽简单，但其突变式的输出变化限制了其应用，特别是在需要连续输出的场景中。

**sigmoid 函数：**
![[Pasted image 20250527143221.png|300]]

$$
h(x) = \frac{1}{1+exp(-x)}
$$
```
def sigmoid(x): 
	return 1 / (1 + np.exp(-x))
```

**神经网络的激活函数必须使用非线性函数。**
为什么不能使用线性函数呢?因为使用线性函数的话,加深神经网络的层数就没有意义了。 线性函数的问题在于,不管如何加深层数,总是存在与之等效的“无隐藏层的神经网络”。

**ReLU 函数：**
$$ 
Relu(x)=
\begin{cases} x, & x>0 \\ 
0, & x<=0 \end{cases} 
$$

```
def relu(x): 
	return np.maximum(0, x)
```

# 3.3 (多维数组) tensor 的理解
```
B = np.array([[1,2], [3,4], [5,6]])
print(B.shape)
>>> (3, 2) 
```
这里的B是一个`3*2`的tensor：
- **第一个维度表示有3个元素，第二个维度表示每个元素有2个特征**
- 同时，**第一个维度对应第 0 维,第二个维度对应第  1 维** 以此类推

**矩阵乘法：** 点积，Hadamard积
[矩阵乘法：点积，Hadamard积，Kronecker积 - 知乎](https://zhuanlan.zhihu.com/p/597645266)
**点积**  最常见的矩阵乘法，有种压缩维度，投影的意思。
```
X = np.array([1, 2])
W = np.array([[1, 3, 5], [2, 4, 6]])
Y = np.dot(X, W) 
print(Y) 

>>> [ 5 11 17]
```
![[Pasted image 20250527145219.png|650]]
**Hadamard积**  两个矩阵必须维度相等，等于两个矩阵对应元相乘，有种参数权重模板的感觉
![[Pasted image 20250527145313.png|650]]

# 3.4  3层神经网络的实现
我们以下图的 3 层神经网络为对象,实现从输入到输出的(前向)处理。神经网络的运算可以作为矩阵运算打包进行
![[Pasted image 20250527145713.png|600]]

**各层间信号传递的实现：** 
- 我们先聚焦于从输入层到第一层的以一个神经元的 信号传递过程
$$
a_1^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)}
$$ 
![[Pasted image 20250527150816.png|200]]
- 使用矩阵运算;
$$\begin{matrix}
A^{(1)} = XW^{(1)} + B^{(1)} \\ 
A^{(1)} = (a_1^{(1)}, a_2^{(1)},a_3^{(1)}) \\ 
X = (x_1, x_2) \\
B^{(1)} = (b_1^{(1)}, b_2^{(1)}, b_3^{(1)}) \\ 
W^{(1)} = \left( \begin{matrix} w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\ w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)} \end{matrix} \right)
\end{matrix}
$$
- 然后得到的结果通过激活函数：sigmoid 函数，得到第一层的输出$Z1$
$$
Z1 = sigmoid(A1)
$$
- 然后第一层的输出变为第二层的输入，以此类推，最后到输出，**输出层所用的激活函数,要根据求解问题的性质决定**，
- 一般地,`回归问题`可以使用恒等函数,`二元分类`问题可以使用 sigmoid 函数,  `多元分类`问题可以使用 softmax 函数。
**代码实现小结：**
```
def init_network(): 
	network = {} # 字典
	network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) 
	network['b1'] = np.array([0.1, 0.2, 0.3]) 
	network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) 
	network['b2'] = np.array([0.1, 0.2]) 
	network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
	network['b3'] = np.array([0.1, 0.2])  
	
	return network

def forward(network, x): 
	W1, W2, W3 = network['W1'], network['W2'], network['W3'] 
	b1, b2, b3 = network['b1'], network['b2'], network['b3']  
	a1 = np.dot(x, W1) + b1 
	z1 = sigmoid(a1) 
	a2 = np.dot(z1, W2) + b2 
	z2 = sigmoid(a2) 
	a3 = np.dot(z2, W3) + b3 
	y = identity_function(a3)  
	
	return y

network = init_network() 
x = np.array([1.0, 0.5]) 
y = forward(network, x) 
print(y) # [ 0.31682708 0.69627909]
```
这里出现了 `forward`(前向)一词,它表示的是从输入到输出方向的传递处理,也称 **前向传播**
当然上述搭建的只是一个网络框架，还不具备学习参数的能力

# 3.5 输出层的设计
神经网络可以用在分类问题和回归问题上,一般而言,回归问题用恒等函数,分类问题用 softmax 函数。
分类问题中使用的 `softmax` 函数可以用下面的式子表示：softmax相当于计算了当前输出的百分比或者说预测类别的'概率'
$$
y_k = \frac{exp(a_k)}{\sum_{i=1}^{n} exp(ai)}
$$
**实现 softmax 函数时的注意事项：** 指数运算可能会导致溢出问题，所以一般做指数运算时会减去输入信号中的最大值（C）
$$
y_k = \frac{exp(a_k-C)}{\sum_{i=1}^{n} exp(ai-C)}
$$**softmax 函数的特征：** 输出总和为 1
**输出层的神经元数量：** 需要根据待解决的问题来决定

# 3.6 手写数字识别
这部分之前实现过，略
不过这里的是纯采用numpy来实现的，还行，有空可以敲一敲

这里调一点关键点说说：
- 求解机器学习问题的步骤(分成学习和推理两个阶段进行)一样,  使用神经网络解决问题时,也需要首先使用训练数据(学习数据)进行权重参数的学习;进行推理时,使用刚才学习到的参数,对输入数据进行分类。