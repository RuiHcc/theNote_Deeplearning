感知机作为神经网络(深度学习)的起源的算法。  
学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。
# 2.1 感知机是什么
**感知机**接收多个输入信号,输出一个信号。感知机的信号只有“流 / 不流”(1/0)两种取值。在本书中,0  对应“不传递信号”,1 对应“传递信号”。
下图是一个接收两个输入信号的感知机的例子：
- x1、x2 是输入信号,   
- y 是 输 出 信 号,
- w1、w2 是权重(weight)。
图中的 ○ 称为“神经元”或者“节点”。输入信号被送往神经元时,会被分别乘以固定的权重(w1x1、w2x2)。神经元会计算传送过来的信号的总和,只有当这个总和超过了某个界限值时,才会输出 1。这也称为“神经元被激活”。这里将这个界限值称为阈值,用符号 θ 表示。
![[Pasted image 20250527133550.png|650]]
数学表达：^1b1fd2
$$
\begin{matrix}
if (w1x1+w2x2<= \theta):y = 0  \\
else : y=1
\end{matrix}
$$ ^a60fcf
# 2.2 简单逻辑电路
这部分内容更多是辅助了解权重，这里以逻辑电路为题材来思考一下与门(AND gate)
**与门：** 仅在两个输入均为 1 时输出 1,其他时候则输出 0
实际上,满足与门的条件的参数的选择方法有无数多个。
	比如 $(w1, w2, θ) = (0.5, 0.5, 0.7)，(0.5, 0.5, 0.8)$
	仅当 x1 和 x2 同时为 1 时,信号的加权总和才会超过给定的阈值 θ
**与非门和或门：**
	要表示与非门,可以用 $(w1, w2, θ) = (−0.5, −0.5, −0.7)$
	或门，$(w1, w2, θ) = (0.5, 0.5, 0.3)$

# 2.3 感知机的实现
**导入权重和偏置：**
为了表示感知机的行为，我们将上式[[#^a60fcf]] 中的 $θ$ 换成 $−b$ 
$$
\begin{matrix}
if (b+w1x1+w2x2<= \theta):y = 0  \\
else : y=1
\end{matrix}
$$
此时,b 称为偏置,w1 和 w2 称为权重。

**使用权重和偏置的实现** 与门
```
def AND(x1, x2): 
x = np.array([x1, x2]) 
w = np.array([0.5, 0.5]) 
b = -0.7 
tmp = np.sum(w*x) + b 

if tmp <= 0: 
	return 0 
else: 
	return 1
```
偏置和权重 w1、w2 的作用是不  一样的。具体地说,
- w1 和 w2 是**控制输入信号的重要性**的参数,
- 而偏置b是**调整神经元被激活的容易程度**(输出信号为 1 的程度)的参数。

# 2.4 感知机的局限性
**异或门：** 仅当 x1 或 x2 中的一方为  1 时,才会输出 1(“异或”是拒绝其他的意思)

| x1  | x2  | y   |
| --- | --- | --- |
| 0   | 0   | 0   |
| 1   | 0   | 1   |
| 0   | 1   | 1   |
| 1   | 1   | 0   |
实际上,用前面介绍的感知机是无法实现这个异或门的,至于为什么，这里用一个图来表示：简单来说感知机只能表示线性模型，而异或门是一个非线性的规律
![[Pasted image 20250527135708.png|650]]

# 2.5 多层感知机 MLP
实际上,感知机的绝妙之处在于它可以“叠加的”，当我们把与门、与非门、或门组合起来就能够表示异或门
```
def XOR(x1, x2): 
	s1 = NAND(x1, x2) 
	s2 = OR(x1, x2) 
	y = AND(s1, s2) 
	return y
```
![[Pasted image 20250527135929.png|600]]

**注意：** 讲到的感知机的局限性,严格地讲,应该是“单层感知机无法  表示异或门”或者“单层感知机无法分离非线性空间”。接下来,我们将看到通过组合感知机(叠加层)就可以实现异或门。

实际上,与门、或门是单层感知机,而异或门是 2 层感知机。叠加了多层的感知机也称为**多层感知机(multi-layered perceptron)**。

感知机通过叠加层能够进行非线性的表示,理论上还可以表示计算机进行的处理