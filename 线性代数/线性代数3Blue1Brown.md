参考：[【熟肉】线性代数的本质 - 00 - “线性代数的本质”系列预览_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1rs411k7ru/?spm_id_from=333.1387.search.video_card.click&vd_source=e67c07b57bd6208ae6cf25baa99d3bcb)
**！！！默认情况下，阐述的观点都是基于二维平面！！！**
**！！！这里更多地是强化对矩阵运算的意义，至于计算可以更多去依赖计算机！！！**
### 矩阵与线性变化
>首先，从向量的角度出发

`a = A*b` 线性变化的本质可以看做是一个函数f(x)，只不过变量为向量，由输入向量转化为输出向量的变化过程，使这个过程发生的是一个矩阵**A**
```
线性：可加性 成比例
```

变换可以理解为是对空间的变化，线性遵循两个原则：1)直线依旧是直线(即保持网格线平行且等距分布)；2)原点保持固定
![[Pasted image 20250218170805.png]]
所以线性变化实际上是对空间的基底进行了改变，所以矩阵向量乘法就是计算线性变化作用于给定向量`[x,y]`的一种途径，这个过程使得原坐标系的基底由`i:[1,0]和j:[0,1]`变成了`i':[a,c]和j':[b,d]`
![[Pasted image 20250218173102.png]]
我们可以把线性变化矩阵的每一列，看做是变化过后的基
![[Pasted image 20250218171518.png]]

>其次，从矩阵的角度出发

当你看到任何一个矩阵时，你都可以把它解读为**对空间的一种特定变换**

### 矩阵乘法与线性变换复合
进一步地，两个矩阵相乘也有其几何意义，也就是两个线性变化的相继作用(类似于复合函数)
![[Pasted image 20250218191322.png]]
但也需要注意，这个过程的**顺序**应该是从右向左的，即先进行旋转矩阵运算，在进行剪切
也因为这个顺序的存在，`M2M1 不等于 M1M2`

此外，也可以从这样一个角度来看，左侧矩阵`Shear`分别作用于两组基：`[0,1]和[-1,0]`然后的到一组新的基底
**一个坐标的左侧方阵可以视为其基底**
这样也就退化成了上一小节，`M2`矩阵分别和`[e,g]和[f,h]`相乘
![[Pasted image 20250218191932.png]]

### 行列式
$$ 
det(A)=
\begin{vmatrix} 
a1&b1&c1\\ 
a2&b2&c2\\ 
a3&b3&c3 
\end{vmatrix} \\ 
=a1(b2c3-b3c2)-b1(a2c3-a3c2)+c1(a2b3-a3b2) $$
上面我们已经提到过，矩阵实际代表了一种线性变化，而对线性变化求行列式，实际意义是：**计算线性变化后基底围成的单位网格面积**

当然也存在下面这种情况：这意味这`A`所代表的变化将空间压缩到了更小的维度
$$ 
det(A)=
\begin{vmatrix} 
2&1\\ 
4&2\\ 
\end{vmatrix} \\ 
=0 $$

![[Pasted image 20250218193624.png]]

但是实际上，折中解释可能存在bug，因为存在行列式为负数的情况啊！
这是因为，(以二维环境举例)我们在变化中，存在将整个空间翻折的变化存在，可以理解为一张纸有着正反两面
具体而言，一般`i`是在`j`取向的顺时针方向，当这个规律被打破时行列式变为负了
$$ 
det(A)=
\begin{vmatrix} 
a&b\\ 
c&d\\ 
\end{vmatrix} \\ 
=ad-cb $$
>事实上`a和d`告诉了我们平行四边形在纵向方向上被拉伸和压缩了多少，而`c和d`告诉了我们平行四边形在对角线方向上被拉伸和压缩了多少

对应的：三维空间中，行列式就代表平行六面体的**体积**了

### 逆矩阵、列空间与零空间
现在我们已经知道了矩阵的用途：对空间进行操作

线性代数回归最基本的用途是求解线性方程组: 
$$
 Ax = v
 $$
![[Pasted image 20250219104120.png]]
这个过程并不简单只是换一种写法这么简单，我们从直观的角度出发，整个过程实际上是在做这样一件事情：**找到一个向量`x`，经过线性变换`A`，使得其与向量`v`重合**
 ![[Pasted image 20250219104405.png]]
 那么这就分为两个情况，一是`det(A)=0`的情况和`det(A)!=0`的情况
 
 先来看看一般`det(A)!=0`的情况：此时有且仅有一个`x`向量与`v`对应
 求解的过程实际是在找一个逆变化`A^-1`，并在逆变化的过程中跟踪`v`的运动
 $$
 x = A^{-1}v
 $$
 所以，只要`det(A)!=0`就能找到唯一的`A^-1`

而当`det(A)=0`的情况下，向量`x`在`A`的作用下被降维成了一条直线，此时没办法找到一个逆变化，你不能将直线转化成一个平面（三维空间同理）
但此时，`x`的解任然可能存在：此时你需要让`v`恰好落在，被降维后的直线上

这时你可能会发现，与三维情况相比，虽然两者变换矩阵的行列式均为0，被压缩到一个平面的情况下，对`x`的要求似乎更加严苛
此时，我们引入一个**秩**：当变换的结果为一条直线时，结果是一维的，我们说该变换矩阵的秩为1；当结果是二维时，秩为2；而与秩为1相比，落在一个平面的向量有更多的可能性，用以回答上面的问题。
这时，是不是更能理解变换后空间的维数代表秩是怎么来的了
`A`无论秩为1/2/3，无论是一条直线、一个平面还是三维空间，所有可能得到的变换结果`Av`的集合，被称为矩阵的**列空间**。

这句话需要好好思考一下，**矩阵的列告诉了基向量变换后的位置，而这些基向量所张成的空间就是所用的变换结果，换句话说，列空间就是矩阵的列所张成的空间**
![[Pasted image 20250219113442.png]]
所以，秩的更精确的定义是列空间的维数；当秩达到最大值，与列数相当，即为**满秩**

零向量一定包含在列空间，因为线性变换原点不变。对于满秩变换来讲，唯一能在变换后落在原点的`Ax=0`就是零向量本身，而非满秩不是(比如二维上，压缩后直线上对角线的向量也能满足，变换后为零向量)

变换后落在原点的向量集合被称为所选矩阵的**零空间或核（kernel）**

举个例子：
对于矩阵A：
$$A=
\left[ \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{matrix} \right]
$$
其列空间为：
$$
Col(A) = c1
\left[ \begin{matrix} 1  \\ 4  \\ 7  \end{matrix} \right]
+ c2
\left[ \begin{matrix} 2  \\ 5  \\ 8  \end{matrix} \right]
+ c3
\left[ \begin{matrix} 3  \\ 6  \\ 9  \end{matrix} \right]
$$

#### 补充- 非方阵

到目前为止，视频中都是采用`2*2`的矩阵来表示二维向量到二维向量的变换或者`3*3`表示三维；那么该如何解读非方阵？
假设给出如下矩阵A，第一列表示其变换后的`i`基底，第二列表示变换后的`j`基底
当我们把一个二维向量`x`给放进去后，会发现`Ax=v`，变成了一个三维空间的向量
注意：这个过程并没有导致维度的上升？他只是被映射到了一个三维空间中
$$
A=
\left[
\begin{matrix}
3 & 4\\
4 & 1\\
5& 9 
\end{matrix}
\right]
$$
这个变换矩阵是三行两列的，即`3*2`的矩阵，这个矩阵空间的列空间，是一个三维空间中的二维平面，但这个矩阵仍然是满秩的，因为其列空间维数和输入空间维数相等
![[Pasted image 20250219141121.png]]
>所以一个`3*2`矩阵的意义是将二维空间映射到三维空间上，矩阵的两列表明输入空间有两个基向量，有三行表示每一个基向量在变换后由三个独立的坐标进行描述

同样，一个`2*3`矩阵表明输入空间是三维的（列空间），三个基向量在变换后仅用两个坐标进行描述，所以这时一个三维到二维的映射

二维到一维同样

### 点积与对偶性
先回顾一下点积的计算过程：
$$
点积：
\left[
\begin{matrix}
a \\
b 
\end{matrix}
\right]
.
\left[
\begin{matrix}
c \\
d 
\end{matrix}
\right]
= ac+bd
= \left[
\begin{matrix}
a & b 
\end{matrix}
\right]
\left[
\begin{matrix}
c \\
d 
\end{matrix}
\right]
$$
我们会发现计算结果恰好等于矩阵乘法；我们都知道点乘由两种定义方式，一种是代数方式，就是ab+cd，第二种是几何方式，就是投影
那么，为什么第二种投影的方式等价于第一种，明明这两种方式的区别巨大并且不直观等价。

我们先来看看投影的过程，如果将投影过程用线性变化的方式进行描述：就是用一个`1*2`的矩阵作用在一个向量上。以`[2 1]`举例，这个变换矩阵变换后都落在一个数轴上，i为2，j为1

我们先假定一个投影变换矩阵`[？,？]`其中？的值，是原来i和j在该数轴上的投影长度，但由于投影的对称性，其数值也等于`u`在i和j上的投影，我们不仿称之为`[ui，uj]`，这便是我们要找的投影矩阵。但是我们会发现把`[ui，uj]`倒过来，这不就是u向量本身吗！
> 所以通过这一串的说明，**投影等价于1x2的线性变换**，**1x2的线性变换等价于点乘的代数定义**，最终解释了为什么点乘的代数定义（计算过程）和几何定义（投影）是等价的。

这其实也带给我们一个思路：在看到变换矩阵时，实际可以想到他背后对应的向量
$$
\left[
\begin{matrix}
a & b 
\end{matrix}
\right]
<-> 
\left[
\begin{matrix}
a \\
b 
\end{matrix}
\right]
$$
![[Pasted image 20250219150735.png]]

###  叉积的标准介绍
$$
v×w = 
\left[
\begin{matrix}
3 \\
1 \\
\end{matrix}
\right]
×
\left[
\begin{matrix}
2 \\
-1 \\
\end{matrix}
\right]
=
det
\left(
\begin{matrix}
3&2 \\
1&-1 \\
\end{matrix}
\right)
$$
$$
a×b = 
\left[
\begin{matrix}
1 \\
2 \\
3
\end{matrix}
\right]
×
\left[
\begin{matrix}
3 \\
4 \\
5
\end{matrix}
\right]
=
det
\left(
\begin{matrix}
i&1&3 \\
j&2&4 \\
k&3&5
\end{matrix}
\right)
$$
两个向量叉积得到的仍是一个向量，该向量遵循右手法则，垂直于另外两个向量，而大小等于围成的面积 
#### 以线性变换的眼光看叉积
如何理解叉积的计算过程和几何含义之间到的关系
$$
v×w = 
\left[
\begin{matrix}
v1 \\
v2 \\
v3
\end{matrix}
\right]
×
\left[
\begin{matrix}
w1 \\
w2 \\
w3
\end{matrix}
\right]
=
det
\left(
\begin{matrix}
x&v1&w1 \\
y&v2&w2 \\
z&v3&w3
\end{matrix}
\right)
$$
当我们说两个向量作叉积时，我们不妨将u=(x,y,z)看做是一个变量，而v和w确定，整个过程是我们输入了一个向量，然后通过行列式输出了一个关于u的数
那么不妨将整个过程写为：
$$
f([x,y,z]) 
=
det
\left(
\begin{matrix}
x&v1&w1 \\
y&v2&w2 \\
z&v3&w3
\end{matrix}
\right)
$$
由于整个函数f是线性的，根据对偶性，所以不妨用一个线性变换（多维到一维）来替换整个函数作用，即存在一个对偶向量p，p点积u 等于 线性变换作用在u上 等价于 求行列式，所以有 `p.u=det(uvw）`

有了以上的假设，首先从计算过程上，我们可以对应求出p的值
$$
\left(
\begin{matrix}
p1 \\
p2 \\
p3
\end{matrix}
\right)
.
\left(
\begin{matrix}
x \\
y \\
z
\end{matrix}
\right)
=
det
\left(
\begin{matrix}
x&v1&w1 \\
y&v2&w2 \\
z&v3&w3
\end{matrix}
\right)
$$

![[Pasted image 20250219170003.png]]

从几何上，`p.u`表示p的值×u在p上的投影，而`det(uvw)`表示uvw三向量构成的六面体体积（u在vw确定平面的垂向投影×vw底面面积），到这里可能还觉得两者的关系比较牵强。
但我们不妨假设垂直于vw平面的向量是q. |xq|×面积=|xp|×|p|. 
此时有无数的p满足等式。 
但是，把p//q这个特殊位置的向量称作叉积。其他虽然能满足等式，但是数值或几何意义不大。 这就与计算过程上建立了联系，形成了呼应

### 基变换
我们已经在前面聊过方阵线性变换的含义了：每一列向量代表新空间的基向量
假设有两个人，他们分别要用自己的坐标系来描述同一向量：其中`[xi,yi],[x0,y0]`分别表示不同坐标描述下的向量。
本质仍是对`i`和`j`的坐标跟踪(用我们的坐标去跟踪，得到A)
$$
A
.
\left[
\begin{matrix}
xi \\
yi \\
\end{matrix}
\right]
=

\left[
\begin{matrix}
x0 \\
y0 \\
\end{matrix}
\right]
$$
![[Pasted image 20250219192639.png]]
![[Pasted image 20250219193126.png]]
所以上面的例子说明了当你看到一个类似`A^-1MA`形式的矩阵时，往往暗示着一种数学上的转移作用，视角上的转换

### 特征向量与特征值
先来解释一下特征向量和特征值的几何意义
首先考虑二维空间的线性变换，比如
$$
\left[
\begin{matrix}
3 & 1\\
0 & 2\\
\end{matrix}
\right]

$$
当我们关注其作用于某一向量时，大部分向量在变换的过程中都偏离了原来的方向![[Pasted image 20250219200139.png]]
但有的向量留在了它原来所张成的空间当中，矩阵对它的作用仅仅是拉伸或缩短了
是否存在矩阵乘法后，只长度缩放，不旋转的点呢？这里的点和向量是同义词。
平面内，除了2条直线上的点没有旋转外(仅长度缩放)，所有其他点都既缩放又旋转了(不考虑原点)。这两条直线对应的就是**特征向量的方向**，缩放比例就是**特征值**。
![[Pasted image 20250219200327.png]]
![[Pasted image 20250219200344.png]]
这些向量就被称为**特征向量**，而拉伸的值就是特征值(衡量特征向量拉伸或压缩的比例因子)，当然也会为负的情况：
$$
Ax =\lambda x
$$
一个较为明显的应用就是：三维空间中的旋转；找到空间中的特征向量(对应特征值为1)即找到了其旋转轴；这作为理解线性变换的关键，且并不依赖于你自己的坐标系

接下来理解一下计算：`det=0`意味着：只有矩阵$(A - \lambda I)$对空间进行了降维，才会有非零向量，满足与之相乘的结果为0

$$
\begin{matrix} 

Av = \lambda v  \\
Av - \lambda Iv = 0 \\
(A - \lambda I)v = 0 \\
det(A - \lambda I) = 0
\end{matrix}
$$
当然也有不存在特征值的情况
#### 特征基
如果基向量恰好都是特征向量会如何？当然这是有条件的
当一个线性变换有足够多的特征向量时，多到足够选出一个张成全空间的集合，就能变换你的坐标系，使得这些特征向量就是基向量
回到开头的例子：$P^{-1}AP = 特征值构成的对角向量$  
$$
\left[
\begin{matrix}
1 & -1\\
0 & 1\\
\end{matrix}
\right]^{-1}

\left[
\begin{matrix}
3 & 1\\
0 & 2\\
\end{matrix}
\right]

\left[
\begin{matrix}
1 & -1\\
0 & 1\\
\end{matrix}
\right]
=
\left[
\begin{matrix}
3 & 0\\
0 & 2\\
\end{matrix}
\right]
$$
这在计算任一变换的多次幂上很有用比如：
$$
\left[
\begin{matrix}
3 & 1\\
0 & 2\\
\end{matrix}
\right]^{100}

$$

